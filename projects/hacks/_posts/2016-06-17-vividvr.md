---
layout: project-post
title: VividVr
date: 2016-06-17
tags: [hackathon,team,ml]
status: publish
type: post
published: true
comments: true
author: andrey_kurenkov
projcategory: hacks
projname: vividvr
excerpt: Method for converting videos to stylized VR worlds
image:
  feature: vivid.gif
  teaser: logo.jpg
what: Combination of algorithms for converting videos to stylized VR worlds
who: Me and Ashish Nair
why: Improvised idea for the 2016 Recurrent Neural Hacks hackathon, seemed cool and possibly doable
where: San Francisco
when: July 17 2016 
links:  
  - name: Presentation
    url: https://docs.google.com/presentation/d/1jhS1tnti-obDVKeLZ6ulVvA_pzSyS4RLn3gwq54LA2c/edit?usp=sharing
  - name: repo
    url: https://github.com/andreykurenkov/vivid-world
  - name: hackathon
    url: http://recurrent-neural-hacks-3961.devpost.com/
images:
  - name: Pres
    file: pres.jpg
    alt: Pres
  - name: Idea
    file: idea.png
    alt: Idea
  - name: Exec
    file: exec.png
    alt: Exec
---
The presentation slides or photos below sum it up; basically we tried to glue together some cutting edge open source projects to get something quite novel. Trying to do so in 24 hours proved tough, and we only go as far as running the separate programs but not to the full pipeline. Something like this may still be worth exploring, though I don't think this is the best approach to go about it. 
